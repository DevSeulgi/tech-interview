# 캐시의 지역성

## 캐시 메모리 (Cache Memory)
캐시 메모리는 속도가 빠른 장치와 느린 장치 간의 속도차에 따른 병목 현상을 줄이기 위한 범용 메모리다. <br>
메인 메모리와 CPU 사이에 위치하며, CPU의 속도에 버금갈 만큼 메모리 계층에서 가장 속도가 빠르지만, 용량이 적고 비싸다는 점도 있다. 

캐시 메모리는 메인 메모리에서 자주 사용하는 프로그램과 데이터를 저장해두어 속도를 빠르게 한다.  <br>
이를 위해서 CPU가 어떤 데이터를 원하는지 어느 정도 예측할 수 있어야 한다. <br>
작은 크기의 캐시 메모리에 CPU가 이후에 참조할 정보가 어느 정도 들어 있는지에 따라 캐시의 성능이 결정되기 때문이다. 

이를 위해 캐시의 지역성(Locality)을 이용한다. 

## 캐시의 지역성 (Cache Locality)
캐시의 지역성이란, 데이터에 대한 접근이 시간적 혹은 공간적으로 가깝게 발생하는 것을 말한다. <br>
캐시의 적중률(Hit rate)을 극대화하여 캐시가 효율적으로 동작하기 위해 사용되는 성질이다. <br>
지역성의 전제조건으로, 프로그램은 모든 코드나 데이터를 균등하게 접근하지 않는다는 특성을 기본으로 한다. <br>
 
캐시의 지역성은 공간 지역성(Spatial Locality)과 시간 지역성(Temporal Locality)으로 나뉜다. 

- 공간 지역성 : 최근에 사용했던 데이터와 인접한 데이터가 참조될 가능성이 높다는 특성
- 시간 지역성 : 최근에 사용했던 데이터가 재참조될 가능성이 높은 특성 

공간 지역성은 배열을 예로 들 수 있다. A[0], A[1]과 같은 연속 접근의 경우 그다음 원소들에 접근할 가능성이 높다. <br>
시간 지역성은 for, while 같은 반복문을 예로 들 수 있다. 특정 부분을 반복해서 접근하기 때문에 다시 참조할 확률이 높다.
## 캐싱 라인 (Caching Line)

<img src="https://user-images.githubusercontent.com/44635266/67989204-66591c80-fc75-11e9-92c1-eef98600a1e0.png" width="800"> <br>

캐시 메모리는 메인 메모리에 비해 크기가 매우 작기 때문에 메인 메모리와 1:1 매칭이 불가능하다. <br>
캐시가 아무리 CPU에 가깝게 위치하더라도, 데이터가 캐시 내의 어느 곳에 저장되어 있는지 찾기가 어려워 모든 데이터를 순회해야 한다면 캐시의 장점을 잃기 때문에 쉽게 찾을 수 있는 구조가 필요하다. <br>
따라서, 캐시에 데이터를 저장할 때 특정 자료구조를 사용하여 묶음으로 저장하는데, 이를 캐싱 라인이라고 한다. <br>
빈번하게 사용되는 데이터의 주소들이 흩어져 있기 때문에 캐시에 저장하는 데이터에는 데이터의 주소 등을 기록해둔 태그를 달아둘 필요가 있다. <br>
이러한 태그들의 묶음을 의미한다. 

캐싱 라인은 다음과 같은 매핑 방법을 사용한다. 

1. 직접 매핑 (Direct Mapping) <br>
<img src="https://user-images.githubusercontent.com/44635266/67989158-475a8a80-fc75-11e9-8854-9cdcef83ac08.png" width="800"> <br>
직접 매핑으로, 메인 메모리를 일정한 크기의 블록으로 나누어 각각의 블록을 캐시의 정해진 위치에 매핑하는 방식이다. 가장 간단하고 구현도 쉽다. <br>
하지만 적중률(Hit rate)이 낮아질 수 있다. <br>
또 동일한 캐시 메모리에 할당된 여러 데이터를 사용할 때 충돌이 발생하게 되는 단점이 있다.
2. 연관 매핑 (Full Associative Mapping) <br>
<img src="https://user-images.githubusercontent.com/44635266/67989160-488bb780-fc75-11e9-975c-2c19e43fb80a.png" width="800"> <br>
캐시 메모리의 빈 공간에 마음대로 주소를 저장하는 방식이다. <br>
저장하는 것은 매우 간단하지만, 원하는 데이터가 있는지 찾기 위해서는 모든 태그를 병렬적으로 검사해야 하기 때문에 복잡하고 비용이 높다는 단점이 있다. 
3. 세트 매핑 (Set Associative Mapping) <br>
<img src="https://user-images.githubusercontent.com/44635266/67989163-49244e00-fc75-11e9-99fa-4b09e5f3276f.png" width="800"> <br>
Direct Mapping과 Full Associative Mapping의 장점을 결합한 방식이다. <br> 
빈 공간에 마음대로 주소를 저장하되, 미리 정해둔 특정 행에만 저장하는 방식이다. <br>
Direct에 비해 검색 속도는 느리지만 저장이 빠르고 Full에 비해 저장이 느리지만 검색은 빠르다. <br>
주로 사용하는 방식이다. 

## 캐시 미스(Cache Miss)
 
캐시 미스는 CPU가 참조하려는 데이터가 캐시 메모리에 없을 때 발생한다. 
 
1. Compulsory Miss <br>
특정 데이터에 처음 접근할 때 발생하는 cache miss이다.
2. Capacity Miss  <br>
캐시 메모리의 공간이 부족해서 발생하는 cache miss이다.
3. Conflict Miss <br>
캐시 메모리에 A와 B 데이터를 저장해야 하는데, A와 B가 같은 캐시 메모리 주소에 할당되어 있어서 발생하는 cache miss이다. direct mapped cache에서 많이 발생한다.

## 캐시의 계층구조

<img src="https://devwhkang.gatsbyjs.io/static/709b21654ba3f63ad5f37cdf62d83a2a/f3efb/CPU-cache.webp" width="800"> <br>

캐시가 시스템 메모리보다 빠르다고 했다. 이는 캐시는 물리적으로 CPU에 가까이 위치해있다. 또한 캐시는 크기가 작기 때문에 빠르다. 시스템 메모리는 보통 4GB, 8GB 등 GB 단위지만 캐시는 몇 MB 이상을 저장하는 경우가 거의 없다. 결국 메모리에서의 주소 비트 수가 더 적기 때문에 CPU가 필요로 하는 데이터 검색이 빠르다. 즉, 캐시에 데이터가 있는지 여부를 쉽게 판별할 수 있다. 시스템 메모리 크기처럼 크게 만들 수도 있겠지만 그러면 당연히 크기가 작은 것에 비해 속도가 느려진다.

그렇다면 캐시를 크게 만들려면 어떻게 하면 될까? 이때 사용되는 방법이 캐시를 계층구조로 나누는 것이다. 최근 마이크로프로세서는 최소한 두세 개의 계층을 가지고 있다. 레벨 1 ~ 레벨 3에 나눠 L1, L2, L3 캐시로 나뉘게 되고 레벨 1부터 CPU에 가깝다. L1 캐시가 가장 작고 빠르며 L3 캐시가 무겁고 크며 느리다. 위에서 언급했듯이 용량이 크면 그만큼 필요한 데이터가 캐시에 있는지 찾기 어렵기 때문에 속도가 느리다.

최근 CPU는 기본적으로 멀티 코어가 탑재되어 있다. 이에 L3 캐시를 지원하는 CPU가 많은데, L3 캐시는 모든 코어가 공유할 수 있는 캐시 메모리이기 때문이다. 그리고 각 코어마다 L1 캐시 L2 캐시를 가지고 있다. 즉, 각 코어마다 L1, L2 캐시가 있고 L3는 공유한다.

CPU는 필요한 데이터에 액세스할 때 가장 가까운 레벨 1에 있는 L1 캐시를 먼저 탐색한다. L1 캐시에 없다면 L2 캐시를 탐색하고 없다면 L3 캐시를 탐색한다. 그래도 없다면 시스템 메모리에서 데이터를 인출하고 다시 캐시에 저장한다. 물론 시스템 메모리 이외에도 SSD, HDD 저장소에도 기록될 수 있다. 하지만 가장 느리고 메모리 주소로 직접 액세스할 수 없다.


> 참고사이트 : 
> - https://rebro.kr/180
> - https://wookcode.tistory.com/183
> - https://devwhkang.gatsbyjs.io/posts/cache/